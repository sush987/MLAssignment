#Batch gradient plotting
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

class LogisticRegression:
    def __init__(self, learning_rate=0.01, num_iterations=100):
        self.learning_rate = learning_rate
        self.num_iterations = num_iterations
        
    def fit(self, X, y):
        # add a column of ones to X for the bias term
        X = np.insert(X, 0, 1, axis=1)
        # initialize the weights to zeros
        self.weights = np.zeros(X.shape[1])
        # initialize the cost history
        self.cost_history = []
        
        for i in range(self.num_iterations):
            # calculate the predicted probabilities
            y_pred = self.sigmoid(np.dot(X, self.weights))
            # calculate the gradient of the cost function
            gradient = np.dot(X.T, (y_pred - y)) / y.size
            # update the weights
            self.weights -= self.learning_rate * gradient
            # calculate the current cost
            cost = self.cost_function(X, y, self.weights)
            # append the current cost to the history
            self.cost_history.append(cost)
        
    def predict(self, X):
        # add a column of ones to X for the bias term
        X = np.insert(X, 0, 1, axis=1)
        # calculate the predicted probabilities
        y_pred = self.sigmoid(np.dot(X, self.weights))
        # round the probabilities to get the predicted class labels
        y_pred_rounded = np.round(y_pred)
        return y_pred_rounded
    
    def sigmoid(self, z):
        z = np.float128(z)
        return 1 / (1 + np.exp(-z))
    
    def cost_function(self, X, y, weights):
        y_pred = self.sigmoid(np.dot(X, weights))
        cost = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))
        return cost

# load the data
#data = np.genfromtxt('data.csv', delimiter=',', skip_header=1)
#X = data[:, :-1]
#y = data[:, -1]
# Load the dataset
url = 'https://drive.google.com/file/d/1iPAmd0wPdbgW9nFlarHXRBLYzFVwvjyK/view?usp=sharing'
file_id = url.split('/')[-2]
dwn_url = 'https://drive.google.com/uc?id=' + file_id
df = pd.read_csv(dwn_url)

# Split the data into features and labels
X = df.drop(['diagnosis'], axis=1).values
y = df['diagnosis'].values

# Convert labels to 0/1
y = np.where(y == 'M', 1, 0)

# normalize the data
X = df.drop(['diagnosis'], axis=1).values
X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
# initialize the logistic regression model with different learning rates
models = []
learning_rates = [0.01, 0.001, 0.0001]
for lr in learning_rates:
    model = LogisticRegression(learning_rate=lr, num_iterations=1000)
    models.append(model)

# train the models
for model in models:
    model.fit(X, y)

# plot the learning curves for each model
plt.figure()
for i, model in enumerate(models):
    plt.plot(range(model.num_iterations), model.cost_history, label='LR={}'.format(learning_rates[i]))
plt.xlabel('Iterations')
plt.ylabel('Cost Function')
plt.title('Learning Curves')
plt.legend()
plt.show()
