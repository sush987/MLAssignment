# -*- coding: utf-8 -*-
"""Neuralnetwork.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s59RUlVfin_EI5iUS5uRa3n0PdL7UDOg
"""

import torch
from torchvision import datasets, transforms
import matplotlib.pyplot as plt

# Transform PIL image into a tensor. The values are in the range [0, 1]
t = transforms.ToTensor()
import torchvision


# Load datasets for training and testing.
mnist_training = datasets.MNIST(root='/tmp/mnist', train=True, download=True, transform=t)
mnist_val = datasets.MNIST(root='/tmp/mnist', train=False, download=True, transform=t)
batch_size = 500
train_loader = torch.utils.data.DataLoader(
    torchvision.datasets.MNIST(root='/tmp/mnist', train=True, download=True,
        transform=torchvision.transforms.Compose([
            torchvision.transforms.ToTensor(),
            torchvision.transforms.Normalize((0.1307,), (0.3081,))
        ])),
    batch_size=batch_size, shuffle=True)

test_loader = torch.utils.data.DataLoader(
    torchvision.datasets.MNIST(root='/tmp/mnist', train=False, download=True,
        transform=torchvision.transforms.Compose([
            torchvision.transforms.ToTensor(),
            torchvision.transforms.Normalize((0.1307,), (0.3081,))
        ])),
    batch_size=batch_size, shuffle=True)

cols = 8
rows = 2

fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(1.5*cols, 2*rows))
for i, ax in enumerate(axes.flatten()):
    image, label = mnist_training[i]          # returns PIL image with its labels
    ax.set_title(f"Label: {label}")
    ax.imshow(image.squeeze(0), cmap='gray')  # we get a 1x28x28 tensor -> remove first dimension
plt.show()

# Create a neural network with two hidden layers, each containing 100 neurons.
model = torch.nn.Sequential(
    torch.nn.Linear(28*28, 100),
    torch.nn.ReLU(),
    torch.nn.Linear(100, 100),
    torch.nn.ReLU(),
    torch.nn.Linear(100, 10)
)

# Use Adam as optimizer.
opt = torch.optim.Adam(params=model.parameters(), lr=0.01)

# Use CrossEntropyLoss for as loss function.
loss_fn = torch.nn.CrossEntropyLoss()

# We train the model with batches of 500 examples.
batch_size = 500
# train_loader = torch.utils.data.DataLoader(mnist_training, batch_size=batch_size, shuffle=True)

# test_loader = torch.utils.data.DataLoader(mnist_val, batch_size=batch_size, shuffle=False)

# Training of the model. We use 10 epochs.
losses = []

for epoch in range(10):
    for imgs, labels in train_loader:
        n = len(imgs)
        # Reshape data from [500, 1, 28, 28] to [500, 784] and use the model to make predictions.
        predictions = model(imgs.view(n, -1))  
        # Compute the loss.
        loss = loss_fn(predictions, labels) 
        opt.zero_grad()
        loss.backward()
        opt.step()
        losses.append(float(loss))
    print(f"Epoch: {epoch}, Loss: {float(loss)}")

# Plot learning curve.
plt.plot(losses)

# Create a simple neural network with two hidden layers, each with 100 neurons.
model = torch.nn.Sequential(
    torch.nn.Linear(784, 100),
    torch.nn.ReLU(),
    torch.nn.Linear(100, 100),
    torch.nn.ReLU(),
    torch.nn.Linear(100, 10)
)

# Use Adam as optimizer.
opt = torch.optim.Adam(params=model.parameters(), lr=0.01)

# Use CrossEntropyLoss for as loss function.
loss_fn = torch.nn.CrossEntropyLoss()

# Train the model.
num_epochs = 10
batch_size = 64

for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        # Flatten input images.
        images = images.view(-1, 28*28)
        
        # Forward pass.
        output = model(images)
        
        # Compute loss.
        loss = loss_fn(output, labels)
        
        # Backward pass.
        opt.zero_grad()
        loss.backward()
        opt.step()

# Initialize variables for computing the confusion matrix.
num_classes = 10
confusion_matrix = torch.zeros(num_classes, num_classes)

# Evaluate the model on the test set.
model.eval()

with torch.no_grad():
    for images, labels in test_loader:
        # Flatten input images.
        images = images.view(-1, 28*28)
        
        # Forward pass.
        output = model(images)
        
        # Compute predicted labels.
        predicted_labels = output.argmax(dim=1)
        
        # Update the confusion matrix.
        for i in range(len(labels)):
            confusion_matrix[labels[i], predicted_labels[i]] += 1

# Compute accuracy and print confusion matrix.
accuracy = confusion_matrix.diag().sum() / confusion_matrix.sum()
print('Accuracy: {:.2f}%'.format(accuracy*100))

print('\nConfusion matrix:')
print(confusion_matrix)

#sigmoid 2 layers 100 neurons
model = torch.nn.Sequential(
    torch.nn.Linear(784, 100),
    torch.nn.Sigmoid(),
    torch.nn.Linear(100, 100),
    torch.nn.Sigmoid(),
    torch.nn.Linear(100, 10)
)


# Use Adam as optimizer.
opt = torch.optim.Adam(params=model.parameters(), lr=0.01)

# Use CrossEntropyLoss for as loss function.
loss_fn = torch.nn.CrossEntropyLoss()

# Train the model.
num_epochs = 10
batch_size = 64

for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        # Flatten input images.
        images = images.view(-1, 28*28)
        
        # Forward pass.
        output = model(images)
        
        # Compute loss.
        loss = loss_fn(output, labels)
        
        # Backward pass.
        opt.zero_grad()
        loss.backward()
        opt.step()

# Initialize variables for computing the confusion matrix.
num_classes = 10
confusion_matrix = torch.zeros(num_classes, num_classes)

# Evaluate the model on the test set.
model.eval()

with torch.no_grad():
    for images, labels in test_loader:
        # Flatten input images.
        images = images.view(-1, 28*28)
        
        # Forward pass.
        output = model(images)
        
        # Compute predicted labels.
        predicted_labels = output.argmax(dim=1)
        
        # Update the confusion matrix.
        for i in range(len(labels)):
            confusion_matrix[labels[i], predicted_labels[i]] += 1

# Compute accuracy and print confusion matrix.
accuracy = confusion_matrix.diag().sum() / confusion_matrix.sum()
print('Accuracy: {:.2f}%'.format(accuracy*100))

print('\nConfusion matrix:')
print(confusion_matrix)

#tanh 2 layers 100 neurons
model = torch.nn.Sequential(
    torch.nn.Linear(784, 100),
    torch.nn.Tanh(),
    torch.nn.Linear(100, 100),
    torch.nn.Tanh(),
    torch.nn.Linear(100, 10)
)


# Use Adam as optimizer.
opt = torch.optim.Adam(params=model.parameters(), lr=0.01)

# Use CrossEntropyLoss for as loss function.
loss_fn = torch.nn.CrossEntropyLoss()

# Train the model.
num_epochs = 10
batch_size = 64

for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        # Flatten input images.
        images = images.view(-1, 28*28)
        
        # Forward pass.
        output = model(images)
        
        # Compute loss.
        loss = loss_fn(output, labels)
        
        # Backward pass.
        opt.zero_grad()
        loss.backward()
        opt.step()

# Initialize variables for computing the confusion matrix.
num_classes = 10
confusion_matrix = torch.zeros(num_classes, num_classes)

# Evaluate the model on the test set.
model.eval()

with torch.no_grad():
    for images, labels in test_loader:
        # Flatten input images.
        images = images.view(-1, 28*28)
        
        # Forward pass.
        output = model(images)
        
        # Compute predicted labels.
        predicted_labels = output.argmax(dim=1)
        
        # Update the confusion matrix.
        for i in range(len(labels)):
            confusion_matrix[labels[i], predicted_labels[i]] += 1

# Compute accuracy and print confusion matrix.
accuracy = confusion_matrix.diag().sum() / confusion_matrix.sum()
print('Accuracy: {:.2f}%'.format(accuracy*100))

print('\nConfusion matrix:')
print(confusion_matrix)

#tanh 2 layers 150 neurons
model = torch.nn.Sequential(
    torch.nn.Linear(784, 150),
    torch.nn.Tanh(),
    torch.nn.Linear(150, 150),
    torch.nn.Tanh(),
    torch.nn.Linear(150, 10)
)


# Use Adam as optimizer.
opt = torch.optim.Adam(params=model.parameters(), lr=0.01)

# Use CrossEntropyLoss for as loss function.
loss_fn = torch.nn.CrossEntropyLoss()

# Train the model.
num_epochs = 10
batch_size = 64

for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        # Flatten input images.
        images = images.view(-1, 28*28)
        
        # Forward pass.
        output = model(images)
        
        # Compute loss.
        loss = loss_fn(output, labels)
        
        # Backward pass.
        opt.zero_grad()
        loss.backward()
        opt.step()

# Initialize variables for computing the confusion matrix.
num_classes = 10
confusion_matrix = torch.zeros(num_classes, num_classes)

# Evaluate the model on the test set.
model.eval()

with torch.no_grad():
    for images, labels in test_loader:
        # Flatten input images.
        images = images.view(-1, 28*28)
        
        # Forward pass.
        output = model(images)
        
        # Compute predicted labels.
        predicted_labels = output.argmax(dim=1)
        
        # Update the confusion matrix.
        for i in range(len(labels)):
            confusion_matrix[labels[i], predicted_labels[i]] += 1

# Compute accuracy and print confusion matrix.
accuracy = confusion_matrix.diag().sum() / confusion_matrix.sum()
print('Accuracy: {:.2f}%'.format(accuracy*100))

print('\nConfusion matrix:')
print(confusion_matrix)

#sigmoid 2 layers 150 neurons
model = torch.nn.Sequential(
    torch.nn.Linear(784, 150),
    torch.nn.Sigmoid(),
    torch.nn.Linear(150, 150),
    torch.nn.Sigmoid(),
    torch.nn.Linear(150, 10)
)


# Use Adam as optimizer.
opt = torch.optim.Adam(params=model.parameters(), lr=0.01)

# Use CrossEntropyLoss for as loss function.
loss_fn = torch.nn.CrossEntropyLoss()

# Train the model.
num_epochs = 10
batch_size = 64

for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        # Flatten input images.
        images = images.view(-1, 28*28)
        
        # Forward pass.
        output = model(images)
        
        # Compute loss.
        loss = loss_fn(output, labels)
        
        # Backward pass.
        opt.zero_grad()
        loss.backward()
        opt.step()

# Initialize variables for computing the confusion matrix.
num_classes = 10
confusion_matrix = torch.zeros(num_classes, num_classes)

# Evaluate the model on the test set.
model.eval()

with torch.no_grad():
    for images, labels in test_loader:
        # Flatten input images.
        images = images.view(-1, 28*28)
        
        # Forward pass.
        output = model(images)
        
        # Compute predicted labels.
        predicted_labels = output.argmax(dim=1)
        
        # Update the confusion matrix.
        for i in range(len(labels)):
            confusion_matrix[labels[i], predicted_labels[i]] += 1

# Compute accuracy and print confusion matrix.
accuracy = confusion_matrix.diag().sum() / confusion_matrix.sum()
print('Accuracy: {:.2f}%'.format(accuracy*100))

print('\nConfusion matrix:')
print(confusion_matrix)

#relu 2 layers 150 neurons
model = torch.nn.Sequential(
    torch.nn.Linear(784, 150),
    torch.nn.ReLU(),
    torch.nn.Linear(150, 150),
    torch.nn.ReLU(),
    torch.nn.Linear(150, 10)
)


# Use Adam as optimizer.
opt = torch.optim.Adam(params=model.parameters(), lr=0.01)

# Use CrossEntropyLoss for as loss function.
loss_fn = torch.nn.CrossEntropyLoss()

# Train the model.
num_epochs = 10
batch_size = 64

for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        # Flatten input images.
        images = images.view(-1, 28*28)
        
        # Forward pass.
        output = model(images)
        
        # Compute loss.
        loss = loss_fn(output, labels)
        
        # Backward pass.
        opt.zero_grad()
        loss.backward()
        opt.step()

# Initialize variables for computing the confusion matrix.
num_classes = 10
confusion_matrix = torch.zeros(num_classes, num_classes)

# Evaluate the model on the test set.
model.eval()

with torch.no_grad():
    for images, labels in test_loader:
        # Flatten input images.
        images = images.view(-1, 28*28)
        
        # Forward pass.
        output = model(images)
        
        # Compute predicted labels.
        predicted_labels = output.argmax(dim=1)
        
        # Update the confusion matrix.
        for i in range(len(labels)):
            confusion_matrix[labels[i], predicted_labels[i]] += 1

# Compute accuracy and print confusion matrix.
accuracy = confusion_matrix.diag().sum() / confusion_matrix.sum()
print('Accuracy: {:.2f}%'.format(accuracy*100))

print('\nConfusion matrix:')
print(confusion_matrix)

#sigmoid 3 layers 100 neurons
model = torch.nn.Sequential(
    torch.nn.Linear(784, 100),
    torch.nn.Sigmoid(),
    torch.nn.Linear(100, 100),
    torch.nn.Sigmoid(),
    torch.nn.Linear(100, 100),
    torch.nn.Sigmoid(),
    torch.nn.Linear(100, 10)
)



# Use Adam as optimizer.
opt = torch.optim.Adam(params=model.parameters(), lr=0.01)

# Use CrossEntropyLoss for as loss function.
loss_fn = torch.nn.CrossEntropyLoss()

# Train the model.
num_epochs = 10
batch_size = 64

for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        # Flatten input images.
        images = images.view(-1, 28*28)
        
        # Forward pass.
        output = model(images)
        
        # Compute loss.
        loss = loss_fn(output, labels)
        
        # Backward pass.
        opt.zero_grad()
        loss.backward()
        opt.step()

# Initialize variables for computing the confusion matrix.
num_classes = 10
confusion_matrix = torch.zeros(num_classes, num_classes)

# Evaluate the model on the test set.
model.eval()

with torch.no_grad():
    for images, labels in test_loader:
        # Flatten input images.
        images = images.view(-1, 28*28)
        
        # Forward pass.
        output = model(images)
        
        # Compute predicted labels.
        predicted_labels = output.argmax(dim=1)
        
        # Update the confusion matrix.
        for i in range(len(labels)):
            confusion_matrix[labels[i], predicted_labels[i]] += 1

# Compute accuracy and print confusion matrix.
accuracy = confusion_matrix.diag().sum() / confusion_matrix.sum()
print('Accuracy: {:.2f}%'.format(accuracy*100))

print('\nConfusion matrix:')
print(confusion_matrix)

#tanh 3 layers 100 neurons
model = torch.nn.Sequential(
    torch.nn.Linear(784, 100),
    torch.nn.Tanh(),
    torch.nn.Linear(100, 100),
    torch.nn.Tanh(),
    torch.nn.Linear(100, 100),
    torch.nn.Tanh(),
    torch.nn.Linear(100, 10)
)



# Use Adam as optimizer.
opt = torch.optim.Adam(params=model.parameters(), lr=0.01)

# Use CrossEntropyLoss for as loss function.
loss_fn = torch.nn.CrossEntropyLoss()

# Train the model.
num_epochs = 10
batch_size = 64

for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        # Flatten input images.
        images = images.view(-1, 28*28)
        
        # Forward pass.
        output = model(images)
        
        # Compute loss.
        loss = loss_fn(output, labels)
        
        # Backward pass.
        opt.zero_grad()
        loss.backward()
        opt.step()

# Initialize variables for computing the confusion matrix.
num_classes = 10
confusion_matrix = torch.zeros(num_classes, num_classes)

# Evaluate the model on the test set.
model.eval()

with torch.no_grad():
    for images, labels in test_loader:
        # Flatten input images.
        images = images.view(-1, 28*28)
        
        # Forward pass.
        output = model(images)
        
        # Compute predicted labels.
        predicted_labels = output.argmax(dim=1)
        
        # Update the confusion matrix.
        for i in range(len(labels)):
            confusion_matrix[labels[i], predicted_labels[i]] += 1

# Compute accuracy and print confusion matrix.
accuracy = confusion_matrix.diag().sum() / confusion_matrix.sum()
print('Accuracy: {:.2f}%'.format(accuracy*100))

print('\nConfusion matrix:')
print(confusion_matrix)

#relu 3 layers 100 neurons
model = torch.nn.Sequential(
    torch.nn.Linear(784, 100),
    torch.nn.ReLU(),
    torch.nn.Linear(100, 100),
    torch.nn.ReLU(),
    torch.nn.Linear(100, 100),
    torch.nn.ReLU(),
    torch.nn.Linear(100, 10)
)



# Use Adam as optimizer.
opt = torch.optim.Adam(params=model.parameters(), lr=0.01)

# Use CrossEntropyLoss for as loss function.
loss_fn = torch.nn.CrossEntropyLoss()

# Train the model.
num_epochs = 10
batch_size = 64

for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        # Flatten input images.
        images = images.view(-1, 28*28)
        
        # Forward pass.
        output = model(images)
        
        # Compute loss.
        loss = loss_fn(output, labels)
        
        # Backward pass.
        opt.zero_grad()
        loss.backward()
        opt.step()

# Initialize variables for computing the confusion matrix.
num_classes = 10
confusion_matrix = torch.zeros(num_classes, num_classes)

# Evaluate the model on the test set.
model.eval()

with torch.no_grad():
    for images, labels in test_loader:
        # Flatten input images.
        images = images.view(-1, 28*28)
        
        # Forward pass.
        output = model(images)
        
        # Compute predicted labels.
        predicted_labels = output.argmax(dim=1)
        
        # Update the confusion matrix.
        for i in range(len(labels)):
            confusion_matrix[labels[i], predicted_labels[i]] += 1

# Compute accuracy and print confusion matrix.
accuracy = confusion_matrix.diag().sum() / confusion_matrix.sum()
print('Accuracy: {:.2f}%'.format(accuracy*100))

print('\nConfusion matrix:')
print(confusion_matrix)

#relu 3 layers 150 neurons
model = torch.nn.Sequential(
    torch.nn.Linear(784, 150),
    torch.nn.ReLU(),
    torch.nn.Linear(150, 150),
    torch.nn.ReLU(),
    torch.nn.Linear(150, 150),
    torch.nn.ReLU(),
    torch.nn.Linear(150, 10)
)



# Use Adam as optimizer.
opt = torch.optim.Adam(params=model.parameters(), lr=0.01)

# Use CrossEntropyLoss for as loss function.
loss_fn = torch.nn.CrossEntropyLoss()

# Train the model.
num_epochs = 10
batch_size = 64

for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        # Flatten input images.
        images = images.view(-1, 28*28)
        
        # Forward pass.
        output = model(images)
        
        # Compute loss.
        loss = loss_fn(output, labels)
        
        # Backward pass.
        opt.zero_grad()
        loss.backward()
        opt.step()

# Initialize variables for computing the confusion matrix.
num_classes = 10
confusion_matrix = torch.zeros(num_classes, num_classes)

# Evaluate the model on the test set.
model.eval()

with torch.no_grad():
    for images, labels in test_loader:
        # Flatten input images.
        images = images.view(-1, 28*28)
        
        # Forward pass.
        output = model(images)
        
        # Compute predicted labels.
        predicted_labels = output.argmax(dim=1)
        
        # Update the confusion matrix.
        for i in range(len(labels)):
            confusion_matrix[labels[i], predicted_labels[i]] += 1

# Compute accuracy and print confusion matrix.
accuracy = confusion_matrix.diag().sum() / confusion_matrix.sum()
print('Accuracy: {:.2f}%'.format(accuracy*100))

print('\nConfusion matrix:')
print(confusion_matrix)

#SIGMOId 3 layers 150 neurons
model = torch.nn.Sequential(
    torch.nn.Linear(784, 150),
    torch.nn.Sigmoid(),
    torch.nn.Linear(150, 150),
    torch.nn.Sigmoid(),
    torch.nn.Linear(150, 150),
    torch.nn.Sigmoid(),
    torch.nn.Linear(150, 10)
)



# Use Adam as optimizer.
opt = torch.optim.Adam(params=model.parameters(), lr=0.01)

# Use CrossEntropyLoss for as loss function.
loss_fn = torch.nn.CrossEntropyLoss()

# Train the model.
num_epochs = 10
batch_size = 64

for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        # Flatten input images.
        images = images.view(-1, 28*28)
        
        # Forward pass.
        output = model(images)
        
        # Compute loss.
        loss = loss_fn(output, labels)
        
        # Backward pass.
        opt.zero_grad()
        loss.backward()
        opt.step()

# Initialize variables for computing the confusion matrix.
num_classes = 10
confusion_matrix = torch.zeros(num_classes, num_classes)

# Evaluate the model on the test set.
model.eval()

with torch.no_grad():
    for images, labels in test_loader:
        # Flatten input images.
        images = images.view(-1, 28*28)
        
        # Forward pass.
        output = model(images)
        
        # Compute predicted labels.
        predicted_labels = output.argmax(dim=1)
        
        # Update the confusion matrix.
        for i in range(len(labels)):
            confusion_matrix[labels[i], predicted_labels[i]] += 1

# Compute accuracy and print confusion matrix.
accuracy = confusion_matrix.diag().sum() / confusion_matrix.sum()
print('Accuracy: {:.2f}%'.format(accuracy*100))

print('\nConfusion matrix:')
print(confusion_matrix)

#tanh 3 layers 150 neurons
model = torch.nn.Sequential(
    torch.nn.Linear(784, 150),
    torch.nn.Tanh(),
    torch.nn.Linear(150, 150),
    torch.nn.Tanh(),
    torch.nn.Linear(150, 150),
    torch.nn.Tanh(),
    torch.nn.Linear(150, 10)
)



# Use Adam as optimizer.
opt = torch.optim.Adam(params=model.parameters(), lr=0.01)

# Use CrossEntropyLoss for as loss function.
loss_fn = torch.nn.CrossEntropyLoss()

# Train the model.
num_epochs = 10
batch_size = 64

for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        # Flatten input images.
        images = images.view(-1, 28*28)
        
        # Forward pass.
        output = model(images)
        
        # Compute loss.
        loss = loss_fn(output, labels)
        
        # Backward pass.
        opt.zero_grad()
        loss.backward()
        opt.step()

# Initialize variables for computing the confusion matrix.
num_classes = 10
confusion_matrix = torch.zeros(num_classes, num_classes)

# Evaluate the model on the test set.
model.eval()

with torch.no_grad():
    for images, labels in test_loader:
        # Flatten input images.
        images = images.view(-1, 28*28)
        
        # Forward pass.
        output = model(images)
        
        # Compute predicted labels.
        predicted_labels = output.argmax(dim=1)
        
        # Update the confusion matrix.
        for i in range(len(labels)):
            confusion_matrix[labels[i], predicted_labels[i]] += 1

# Compute accuracy and print confusion matrix.
accuracy = confusion_matrix.diag().sum() / confusion_matrix.sum()
print('Accuracy: {:.2f}%'.format(accuracy*100))

print('\nConfusion matrix:')
print(confusion_matrix)

#tanh 3 layers 125 neurons
model = torch.nn.Sequential(
    torch.nn.Linear(784, 125),
    torch.nn.Tanh(),
    torch.nn.Linear(125, 125),
    torch.nn.Tanh(),
    torch.nn.Linear(125, 125),
    torch.nn.Tanh(),
    torch.nn.Linear(125, 10)
)



# Use Adam as optimizer.
opt = torch.optim.Adam(params=model.parameters(), lr=0.01)

# Use CrossEntropyLoss for as loss function.
loss_fn = torch.nn.CrossEntropyLoss()

# Train the model.
num_epochs = 10
batch_size = 64

for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        # Flatten input images.
        images = images.view(-1, 28*28)
        
        # Forward pass.
        output = model(images)
        
        # Compute loss.
        loss = loss_fn(output, labels)
        
        # Backward pass.
        opt.zero_grad()
        loss.backward()
        opt.step()

# Initialize variables for computing the confusion matrix.
num_classes = 10
confusion_matrix = torch.zeros(num_classes, num_classes)

# Evaluate the model on the test set.
model.eval()

with torch.no_grad():
    for images, labels in test_loader:
        # Flatten input images.
        images = images.view(-1, 28*28)
        
        # Forward pass.
        output = model(images)
        
        # Compute predicted labels.
        predicted_labels = output.argmax(dim=1)
        
        # Update the confusion matrix.
        for i in range(len(labels)):
            confusion_matrix[labels[i], predicted_labels[i]] += 1

# Compute accuracy and print confusion matrix.
accuracy = confusion_matrix.diag().sum() / confusion_matrix.sum()
print('Accuracy: {:.2f}%'.format(accuracy*100))

print('\nConfusion matrix:')
print(confusion_matrix)

#sigmoid 3 layers 125 neurons
model = torch.nn.Sequential(
    torch.nn.Linear(784, 125),
    torch.nn.Sigmoid(),
    torch.nn.Linear(125, 125),
    torch.nn.Sigmoid(),
    torch.nn.Linear(125, 125),
    torch.nn.Sigmoid(),
    torch.nn.Linear(125, 10)
)



# Use Adam as optimizer.
opt = torch.optim.Adam(params=model.parameters(), lr=0.01)

# Use CrossEntropyLoss for as loss function.
loss_fn = torch.nn.CrossEntropyLoss()

# Train the model.
num_epochs = 10
batch_size = 64

for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        # Flatten input images.
        images = images.view(-1, 28*28)
        
        # Forward pass.
        output = model(images)
        
        # Compute loss.
        loss = loss_fn(output, labels)
        
        # Backward pass.
        opt.zero_grad()
        loss.backward()
        opt.step()

# Initialize variables for computing the confusion matrix.
num_classes = 10
confusion_matrix = torch.zeros(num_classes, num_classes)

# Evaluate the model on the test set.
model.eval()

with torch.no_grad():
    for images, labels in test_loader:
        # Flatten input images.
        images = images.view(-1, 28*28)
        
        # Forward pass.
        output = model(images)
        
        # Compute predicted labels.
        predicted_labels = output.argmax(dim=1)
        
        # Update the confusion matrix.
        for i in range(len(labels)):
            confusion_matrix[labels[i], predicted_labels[i]] += 1

# Compute accuracy and print confusion matrix.
accuracy = confusion_matrix.diag().sum() / confusion_matrix.sum()
print('Accuracy: {:.2f}%'.format(accuracy*100))

print('\nConfusion matrix:')
print(confusion_matrix)

#relu 3 layers 125 neurons
model = torch.nn.Sequential(
    torch.nn.Linear(784, 125),
    torch.nn.ReLU(),
    torch.nn.Linear(125, 125),
    torch.nn.ReLU(),
    torch.nn.Linear(125, 125),
    torch.nn.ReLU(),
    torch.nn.Linear(125, 10)
)



# Use Adam as optimizer.
opt = torch.optim.Adam(params=model.parameters(), lr=0.01)

# Use CrossEntropyLoss for as loss function.
loss_fn = torch.nn.CrossEntropyLoss()

# Train the model.
num_epochs = 10
batch_size = 64

for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        # Flatten input images.
        images = images.view(-1, 28*28)
        
        # Forward pass.
        output = model(images)
        
        # Compute loss.
        loss = loss_fn(output, labels)
        
        # Backward pass.
        opt.zero_grad()
        loss.backward()
        opt.step()

# Initialize variables for computing the confusion matrix.
num_classes = 10
confusion_matrix = torch.zeros(num_classes, num_classes)

# Evaluate the model on the test set.
model.eval()

with torch.no_grad():
    for images, labels in test_loader:
        # Flatten input images.
        images = images.view(-1, 28*28)
        
        # Forward pass.
        output = model(images)
        
        # Compute predicted labels.
        predicted_labels = output.argmax(dim=1)
        
        # Update the confusion matrix.
        for i in range(len(labels)):
            confusion_matrix[labels[i], predicted_labels[i]] += 1

# Compute accuracy and print confusion matrix.
accuracy = confusion_matrix.diag().sum() / confusion_matrix.sum()
print('Accuracy: {:.2f}%'.format(accuracy*100))

print('\nConfusion matrix:')
print(confusion_matrix)