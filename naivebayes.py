# -*- coding: utf-8 -*-
"""NaiveBayes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DJEm3OJP4pVYxrNguUYZYEatKQEUHDCx
"""

import pandas as pd

# Read the data from the uploaded Notepad file
df = pd.read_csv('/content/adult.data', delimiter=', ')

# Write the data to an Excel file
df.to_excel('/content/data.xlsx', index=False)

#Task1
import numpy as np
import pandas as pd

def preprocess_data(df):
    # Replace missing values with the mode of the data in that column
    for col in df.columns:
        if df[col].dtype == object and (df[col] == ' ?').any():
            mode = df[col][df[col] != ' ?'].mode().iloc[0]
            df[col].replace(' ?', mode, inplace=True)
        elif df[col].isna().any():
          df.fillna(df.mean(), inplace=True)
            # df[col].fillna(df[col].mean(), inplace=True)

            
    # Convert categorical variables to numeric values
    # for col in df.columns:
    #     if df[col].dtype == object:
    #         df[col] = pd.Categorical(df[col])
    #         df[col] = df[col].cat.codes

    return df

# Read the data from the Excel file
df = pd.read_excel('data.xlsx')

# Replace missing values with the average of the data in that column
# df.fillna(df.mean(), inplace=True)

sample_data = df.columns.tolist()
df = df.iloc[1:, :]
arr =['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country','income ']
# print(len(arr))
# print(len(df.iloc[0]))
df.iloc[0] = arr 
df = preprocess_data(df) 
# Write the modified dataframe back to the Excel file
# df.to_excel('path/to/excel/file.xlsx', index=False)
# Write the data to an Excel 
# df.to_excel('data_filled.xlsx', index=False, header=['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country'])
df.to_excel('data_filled.xlsx', index=False)

import pandas as pd
import random

# Set the random seed for reproducibility
random.seed(42)

# Read the data from the Excel file
df = pd.read_excel('data_filled.xlsx')

# Shuffle the rows of the data
df = df.sample(frac=1).reset_index(drop=True)

# Split the data into training and testing sets
train_size = int(0.8 * len(df))
train_data = df.iloc[:train_size].reset_index(drop=True)
test_data = df.iloc[train_size:].reset_index(drop=True)

# Write the training and testing sets to Excel files
train_data.to_excel('train_data.xlsx', index=False)
test_data.to_excel('test_data.xlsx', index=False)



def calculate_prior_probabilities(labels, classes):
    """
    Calculate the prior probabilities of each class.
    
    Args:
    labels: A Pandas Series representing the labels.
    classes: A list of the unique classes in the labels.
    
    Returns:
    A dictionary containing the prior probability of each class.
    """
    priors = {}
    for c in classes:
        priors[c] = (labels == c).sum() / len(labels)
    return priors

features = train_data.iloc[:, :-1]
labels = train_data.iloc[:, -1]

# Calculate the prior probabilities of each class
priors = calculate_prior_probabilities(labels, ['<=50K', '>50K'])

# Print the prior probabilities
print(priors)

import numpy as np

def calculate_conditional_probabilities(train_data, labels):
    # Get the number of features
    num_features = train_data.shape[1] - 1

    # Get the unique classes
    classes = np.unique(labels)

    # Calculate the conditional probabilities for each feature and class
    conditional_probabilities = {}
    for c in classes:
        # Get the rows corresponding to this class
        class_rows = train_data.loc[labels == c].iloc[:, :-1]

        # Calculate the conditional probabilities for each feature
        conditional_probabilities[c] = {}
        for i in range(num_features):
            feature_values = np.unique(class_rows.iloc[:, i])
            feature_probabilities = {}
            for value in feature_values:
                count = len(class_rows[class_rows.iloc[:, i] == value])
                feature_probabilities[value] = count / float(len(class_rows))
            conditional_probabilities[c][i] = feature_probabilities

    return conditional_probabilities

# call the function for train_data
# labels = train_data.iloc[:, -1].values

conditional_probabilities = calculate_conditional_probabilities(train_data, labels)
print(conditional_probabilities)

def predict_class(instance, prior_probabilities, conditional_probabilities):
    """
    Predicts the class of a given instance using the Naive Bayes algorithm.
    
    Args:
    - instance (numpy array): a 1D numpy array representing the instance to be classified
    - prior_probabilities (dictionary): a dictionary containing the prior probabilities of each class
    - conditional_probabilities (dictionary): a dictionary containing the conditional probabilities of each feature given each class
    
    Returns:

    - predicted_class: the predicted class of the given instance
    """
    # Initialize variables
    max_probability = -1
    predicted_class = None
    
    # Iterate over each class
    for class_label in prior_probabilities.keys():
        
        # Calculate the class probability
        class_probability = prior_probabilities[class_label]
        
        # Calculate the product of the conditional probabilities of each feature
        for i in range(len(instance)):
            feature_value = instance[i]
            # print(len(instance))
            # print(i)
            feature_probabilities = conditional_probabilities[class_label][i]
            
            # Find the probability of the feature value given the class
            if feature_value in feature_probabilities:
                feature_probability = feature_probabilities[feature_value]
            else:
                feature_probability = 0
                
            # Multiply the feature probability by the class probability
            class_probability *= feature_probability
        
        # Check if the class probability is greater than the maximum probability seen so far
        if class_probability > max_probability:
            max_probability = class_probability
            predicted_class = class_label
            
    return predicted_class

# print(priors.keys())
# print(conditional_probabilities['<=50K'][0])
test_features = test_data.iloc[:, :-1]
test_labels = test_data.iloc[:, -1]
instance = test_features.iloc[0, :-1].values

predicted_class = predict_class(instance, priors, conditional_probabilities)
# print(predicted_class)
print("Predicted class:", predicted_class)
print("True class:", test_labels.iloc[0])

import numpy as np

def evaluate_performance(y_true, y_pred):
    # Convert y_true and y_pred to arrays if they are not already arrays
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)

    # Calculate the confusion matrix
    # 0 is '<=50K'
    # 1 is '>50K'
    TP = np.count_nonzero((y_true == '>50K') & (y_pred == '>50K'))
    TN = np.count_nonzero((y_true == '<=50K') & (y_pred == '<=50K'))
    FP = np.count_nonzero((y_true == '<=50K') & (y_pred == '>50K'))
    FN = np.count_nonzero((y_true == '>50K') & (y_pred == '<=50K'))

    # Calculate accuracy
    accuracy = (TP + TN) / (TP + TN + FP + FN)

    # Calculate precision
    precision = TP / (TP + FP)

    # Calculate recall
    recall = TP / (TP + FN)

    # Calculate F1-score
    f1_score = 2 * (precision * recall) / (precision + recall)

    return accuracy, precision, recall, f1_score

# predicted_labels = []
# instance = test_features.iloc[0, :-1].values
# for row in test_data:
#     predicted_label = predict_class(row[:-1], priors, conditional_probabilities)
#     predicted_labels.append(predicted_label)
predicted_labels = []
for i in range(len(test_data)):
    row = test_data.iloc[i]
    predicted_label = predict_class(row[:-1], priors, conditional_probabilities)
    predicted_labels.append(predicted_label)

# print(predicted_labels.dtype())
# predicted_labels = ['>50K','<=50K','>50K','<=50K']
# daa = ['<=50K','<=50K','>50K','<=50K']
y_true = test_data.iloc[:, -1].tolist()
# print(len(y_true))
# print(len(predicted_labels))
accuracy, precision, recall, f1_score = evaluate_performance(y_true, predicted_labels)
# accuracy, precision, recall, f1_score = evaluate_performance(daa, predicted_labels)
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1_score)

#laplace smoothening
import numpy as np

def calculate_laplace_conditional_probabilities(train_data, labels, alpha):
    # Get the number of features
    num_features = train_data.shape[1] - 1
    # Get the unique classes
    classes = np.unique(labels)
    # alpha = 1
    # Calculate the conditional probabilities for each feature and class
    conditional_probabilities = {}
    for c in classes:
        # Get the rows corresponding to this class
        class_rows = train_data[train_data.iloc[:, -1] == c]

        # Calculate the conditional probabilities for each feature
        conditional_probabilities[c] = {}
        for i in range(num_features):
            # feature_values = np.unique(class_rows[:, i])
            feature_values = np.unique(class_rows.iloc[:, i])
            feature_probabilities = {}
            for value in feature_values:
                # count = len(class_rows[class_rows[:, i] == value])
                count = len(class_rows[class_rows.iloc[:, i] == value])
                prob = (count + alpha) / float(len(class_rows) + alpha * len(feature_values))
                feature_probabilities[value] = prob
            conditional_probabilities[c][i] = feature_probabilities

    return conditional_probabilities

# Train the model with Laplace smoothing
conditional_laplace_probabilities = calculate_laplace_conditional_probabilities(train_data, labels, 1)

# Use the trained model to predict the test set
# predicted = predict_class(test_data, conditional_laplace_probabilities)
# predicted = predict_class(test_data[:, :-1], priors, conditional_laplace_probabilities)
predicted_labels = []
for i in range(len(test_data)):
    row = test_data.iloc[i]
    predicted_label = predict_class(row[:-1], priors, conditional_laplace_probabilities)
    predicted_labels.append(predicted_label)

# Calculate the accuracy of the model
# accuracy = calculate_accuracy(predicted, test_labels)

# Print the accuracy
# predicted_labels = predict_class(test_data[:, :-1], prior_probabilities, conditional_probabilities)
y_true = test_data.iloc[:, -1].tolist()
accuracy, precision, recall, f1_score = evaluate_performance(y_true, predicted_labels)
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1_score)

# Train the model with add-k  smoothing
conditional_laplace_probabilities = calculate_laplace_conditional_probabilities(train_data, labels, 0.1)

# Use the trained model to predict the test set
# predicted = predict_class(test_data, conditional_laplace_probabilities)
# predicted = predict_class(test_data[:, :-1], priors, conditional_laplace_probabilities)
predicted_labels = []
for i in range(len(test_data)):
    row = test_data.iloc[i]
    predicted_label = predict_class(row[:-1], priors, conditional_laplace_probabilities)
    predicted_labels.append(predicted_label)

# Calculate the accuracy of the model
# accuracy = calculate_accuracy(predicted, test_labels)

# Print the accuracy
# predicted_labels = predict_class(test_data[:, :-1], prior_probabilities, conditional_probabilities)
y_true = test_data.iloc[:, -1].tolist()
accuracy, precision, recall, f1_score = evaluate_performance(y_true, predicted_labels)
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1_score)

import numpy as np

# Set random seed for reproducibility
np.random.seed(42)

# Define number of splits to create
n_splits = 10

# Initialize arrays to store performance metrics for each split
accuracies = np.zeros(n_splits)
precisions = np.zeros(n_splits)
recalls = np.zeros(n_splits)
f1_scores = np.zeros(n_splits)

# Loop over the number of splits and create training and testing indices
for i in range(n_splits):
    # Create random indices for the training and testing sets
    train_indices = np.random.choice(len(train_data), int(0.8 * len(train_data)), replace=False)
    test_indices = np.array(list(set(range(len(train_data))) - set(train_indices)))
    
    # Split the data into training and testing sets
    X_train = train_data.iloc[train_indices, :-1]
    y_train = train_data.iloc[train_indices, -1]
    X_test = train_data.iloc[test_indices, :-1]
    y_test = train_data.iloc[test_indices, -1]
    

    y_pred = []
    for j in range(len(X_test)):
        row = X_test.iloc[j]
        predicted_label = predict_class(row[:-1], priors, conditional_probabilities)
        y_pred.append(predicted_label)
    
    # Calculate performance metrics and store them in the arrays
    accuracies[i], precisions[i], recalls[i], f1_scores[i] = evaluate_performance(y_true, predicted_labels)

print(f"Average accuracy: {np.mean(accuracies):.3f} +/- {np.std(accuracies):.3f}")
print(f"Average precision: {np.mean(precisions):.3f} +/- {np.std(precisions):.3f}")
print(f"Average recall: {np.mean(recalls):.3f} +/- {np.std(recalls):.3f}")
print(f"Average F1 score: {np.mean(f1_scores):.3f} +/- {np.std(f1_scores):.3f}")

#logistic
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix

# Load the data
url = 'https://drive.google.com/file/d/1sNZOfsRY0nDPkl1SeRibfjGea7ZABNpv/view?usp=sharing'
file_id = url.split('/')[-2]
dwn_url = 'https://drive.google.com/uc?id=' + file_id
df = pd.read_csv(dwn_url, header=None)

# Convert string columns to numeric
le = LabelEncoder()
for i in range(len(df.columns)):
    if df[i].dtype == 'object':
        df[i] = le.fit_transform(df[i])

# Split the data into training and testing sets
X = df.iloc[:, :-1].values
y = df.iloc[:, -1].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize the feature data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train the logistic regression model
lr = LogisticRegression(random_state=42)
lr.fit(X_train, y_train)

# Predict on the test set
y_pred = lr.predict(X_test)

# Calculate accuracy, precision, recall, and confusion matrix
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

print('Accuracy:', accuracy)
print('Precision:', precision)
print('Recall:', recall)
print('Confusion Matrix:\n', cm)

#KNN
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score

# Load the data
url = 'https://drive.google.com/file/d/1sNZOfsRY0nDPkl1SeRibfjGea7ZABNpv/view?usp=sharing'
file_id = url.split('/')[-2]
dwn_url = 'https://drive.google.com/uc?id=' + file_id
df = pd.read_csv(dwn_url, header=None)

# Convert string columns to numeric
le = LabelEncoder()
for i in range(len(df.columns)):
    if df[i].dtype == 'object':
        df[i] = le.fit_transform(df[i])

# Split the data into training and testing sets
X = df.iloc[:, :-1].values
y = df.iloc[:, -1].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Scale the numerical variables
sc = StandardScaler()
X_train[:, :4] = sc.fit_transform(X_train[:, :4])
X_test[:, :4] = sc.transform(X_test[:, :4])

# Fit the KNN model
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

# Predict the target variable on the test set
y_pred = knn.predict(X_test)


# Calculate accuracy, precision, recall, and confusion matrix
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
print('Accuracy:', accuracy)
print('Precision:', precision)
print('Recall:', recall)
print('F1 Score:', f1)
print('Confusion Matrix:\n', cm)

#logistic with no library
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler
# Define the sigmoid function
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Define the logistic regression model
class LogisticRegression:
    def __init__(self, lr=0.01, num_iterations=1000):
        self.lr = lr
        self.num_iterations = num_iterations
    
    def fit(self, X, y):
        m, n = X.shape
        self.theta = np.zeros((n, 1))
        
        # Gradient descent
        for i in range(self.num_iterations):
            z = np.dot(X, self.theta)
            h = sigmoid(z)
            gradient = np.dot(X.T, (h - y)) / m
            self.theta -= self.lr * gradient
    
    def predict(self, X):
        z = np.dot(X, self.theta)
        h = sigmoid(z)
        return np.round(h)
    
    def predict_proba(self, X):
        z = np.dot(X, self.theta)
        h = sigmoid(z)
        return h
        
# Load the data
url = 'https://drive.google.com/file/d/1sNZOfsRY0nDPkl1SeRibfjGea7ZABNpv/view?usp=sharing'
file_id = url.split('/')[-2]
dwn_url = 'https://drive.google.com/uc?id=' + file_id
df = pd.read_csv(dwn_url, header=None)

# Convert string columns to numeric
le = LabelEncoder()
for i in range(len(df.columns)):
    if df[i].dtype == 'object':
        df[i] = le.fit_transform(df[i])

# Split the data into training and testing sets
X = df.iloc[:, :-1].values
y = df.iloc[:, -1].values.reshape(-1, 1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize the feature data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train the logistic regression model
lr = LogisticRegression(lr=0.01, num_iterations=1000)
lr.fit(X_train, y_train)

# Predict on the test set
y_pred = lr.predict(X_test)
y_proba = lr.predict_proba(X_test)

# Calculate accuracy, precision, recall, and confusion matrix
tp = np.sum((y_test == 1) & (y_pred == 1))
fp = np.sum((y_test == 0) & (y_pred == 1))
tn = np.sum((y_test == 0) & (y_pred == 0))
fn = np.sum((y_test == 1) & (y_pred == 0))

accuracy = (tp + tn) / (tp + fp + tn + fn)
precision = tp / (tp + fp)
recall = tp / (tp + fn)
f1_score = 2 * (precision * recall) / (precision + recall)

print('Accuracy:', accuracy)
print('Precision:', precision)
print('Recall:', recall)
print('F1 Score:', f1_score)
print('Confusion Matrix:\n', np.array([[tn, fp], [fn, tp]]))